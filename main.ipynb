{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the SRGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# builtin \n",
    "import glob\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# all imports\n",
    "import torch \n",
    "import numpy as np \n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# our modules\n",
    "from src.config import cfg, root_path\n",
    "from src.utils import MeanSTDFinder\n",
    "from src.data_loaders import SuperResolutionDataLoader\n",
    "from src.models.srgan import Generator, Discriminator, VggFeatureExtractor\n",
    "\n",
    "\n",
    "# create path for models checkpoint\n",
    "Path(root_path).joinpath(\"saved_models/srgan\").mkdir(exist_ok=True, parents=True)\n",
    "Path(root_path).joinpath(\"saved_models/srgan/images\").mkdir(exist_ok=True, parents=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the images dataset path \n",
    "images_pth = cfg.dataset.images_dir\n",
    "\n",
    "train_paths, test_paths = train_test_split(\n",
    "    sorted(glob.glob(images_pth + \"/*.*\")),\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# get the mean and std of the dataset \n",
    "# mean_std = MeanSTDFinder(images_dir=images_pth)()\n",
    "mean_std = {'mean': [0.2903465 , 0.31224626, 0.29810828],\n",
    " 'std': [0.1457739 , 0.13011318, 0.12317199]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataloaders\n",
    "train_dataloader = DataLoader(\n",
    "    SuperResolutionDataLoader(train_paths,**mean_std),\n",
    "    batch_size=cfg.train.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=cfg.train.n_cpu,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    SuperResolutionDataLoader(test_paths,**mean_std),\n",
    "    batch_size=int(cfg.train.batch_size * 0.75),\n",
    "    shuffle=True,\n",
    "    num_workers=cfg.train.n_cpu,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def gaussian(window_size, sigma):\n",
    "    gauss = torch.Tensor([exp(-(x - window_size // 2) ** 2 / float(2 * sigma ** 2)) for x in range(window_size)])\n",
    "    return gauss / gauss.sum()\n",
    "\n",
    "\n",
    "def create_window(window_size, channel):\n",
    "    _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
    "    _2D_window = _1D_window.mm(_1D_window.t()).float().unsqueeze(0).unsqueeze(0)\n",
    "    window = Variable(_2D_window.expand(channel, 1, window_size, window_size).contiguous())\n",
    "    return window\n",
    "\n",
    "\n",
    "def _ssim(img1, img2, window, window_size, channel, size_average=True):\n",
    "    mu1 = F.conv2d(img1, window, padding=window_size // 2, groups=channel)\n",
    "    mu2 = F.conv2d(img2, window, padding=window_size // 2, groups=channel)\n",
    "\n",
    "    mu1_sq = mu1.pow(2)\n",
    "    mu2_sq = mu2.pow(2)\n",
    "    mu1_mu2 = mu1 * mu2\n",
    "\n",
    "    sigma1_sq = F.conv2d(img1 * img1, window, padding=window_size // 2, groups=channel) - mu1_sq\n",
    "    sigma2_sq = F.conv2d(img2 * img2, window, padding=window_size // 2, groups=channel) - mu2_sq\n",
    "    sigma12 = F.conv2d(img1 * img2, window, padding=window_size // 2, groups=channel) - mu1_mu2\n",
    "\n",
    "    C1 = 0.01 ** 2\n",
    "    C2 = 0.03 ** 2\n",
    "\n",
    "    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))\n",
    "\n",
    "    if size_average:\n",
    "        return ssim_map.mean()\n",
    "    else:\n",
    "        return ssim_map.mean(1).mean(1).mean(1)\n",
    "\n",
    "def ssim(img1, img2, window_size=11, size_average=True):\n",
    "    (_, channel, _, _) = img1.size()\n",
    "    window = create_window(window_size, channel)\n",
    "\n",
    "    if img1.is_cuda:\n",
    "        window = window.cuda(img1.get_device())\n",
    "    window = window.type_as(img1)\n",
    "\n",
    "    return _ssim(img1, img2, window, window_size, channel, size_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Define the Model Parameters ##########\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "feature_extractor = VggFeatureExtractor()\n",
    "feature_extractor.eval()\n",
    "\n",
    "gan_loss = torch.nn.BCEWithLogitsLoss()\n",
    "content_loss = torch.nn.L1Loss()\n",
    "\n",
    "\n",
    "# Transfer all to the device\n",
    "generator = generator.to(cfg.device.device)\n",
    "discriminator = discriminator.to(cfg.device.device)\n",
    "feature_extractor = feature_extractor.to(cfg.device.device)\n",
    "gan_loss = gan_loss.to(cfg.device.device)\n",
    "content_loss = content_loss.to(cfg.device.device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the optimizers for generator and discriminator\n",
    "\n",
    "optimizer_G = torch.optim.Adam(\n",
    "    generator.parameters(),\n",
    "    lr=cfg.train.learning_rate,\n",
    "    betas=(cfg.train.b1, cfg.train.b2),\n",
    ")\n",
    "optimizer_D = torch.optim.Adam(\n",
    "    discriminator.parameters(),\n",
    "    lr=cfg.train.learning_rate,\n",
    "    betas=(cfg.train.b1, cfg.train.b2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|â–Ž         | 6/166 [00:22<10:04,  3.78s/it, disc_loss=0.633, gen_loss=1.31]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 64\u001b[0m\n\u001b[1;32m     60\u001b[0m optimizer_D\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m################## Accumulate losses ###############\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m gen_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mtotal_loss_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m disc_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m total_disc_loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     67\u001b[0m train_bar\u001b[38;5;241m.\u001b[39mset_postfix(\n\u001b[1;32m     68\u001b[0m     gen_loss\u001b[38;5;241m=\u001b[39mgen_loss \u001b[38;5;241m/\u001b[39m (batch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), disc_loss\u001b[38;5;241m=\u001b[39mdisc_loss \u001b[38;5;241m/\u001b[39m (batch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     69\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# train losses\n",
    "train_gen_loss, train_disc_loss, train_counter = [], [], []\n",
    "# test losses\n",
    "test_gen_loss, test_disc_loss = [], []\n",
    "test_ssim = []\n",
    "test_psnr = []\n",
    "\n",
    "\n",
    "for epoch in range(cfg.train.n_epochs):\n",
    "\n",
    "    ############################ Training ####################\n",
    "    gen_loss = 0\n",
    "    disc_loss = 0\n",
    "    train_bar = tqdm(train_dataloader, desc=f\"Training\")\n",
    "\n",
    "    for batch_idx, imgs in enumerate(train_bar):\n",
    "\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "\n",
    "        low_res_ipt = imgs[\"lr\"].to(cfg.device.device)\n",
    "        high_res_ipt = imgs[\"hr\"].to(cfg.device.device)\n",
    "        #################### Generator ######################\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "        generated_hr = generator(low_res_ipt)\n",
    "        disc_opt = discriminator(generated_hr)\n",
    "\n",
    "        # Adverserial loss\n",
    "        loss_GAN = gan_loss(disc_opt, torch.ones_like(disc_opt))\n",
    "\n",
    "        # content loss\n",
    "        generated_features = feature_extractor(generated_hr)\n",
    "        real_feaures = feature_extractor(high_res_ipt)\n",
    "        loss_CONTENT = content_loss(generated_features, real_feaures)\n",
    "\n",
    "        # total loss\n",
    "        total_loss_generator = loss_CONTENT + 1e-3 * loss_GAN\n",
    "\n",
    "        # backpropagate\n",
    "        total_loss_generator.backward()\n",
    "        optimizer_G.step()\n",
    "        #################### discriminator ######################\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        real_disc_opt = discriminator(high_res_ipt)\n",
    "        loss_D_real = gan_loss(real_disc_opt, torch.ones_like(real_disc_opt))\n",
    "\n",
    "        fake_disc_opt = discriminator(generated_hr.detach())\n",
    "        loss_D_fake = gan_loss(fake_disc_opt, torch.zeros_like(fake_disc_opt))\n",
    "\n",
    "        # total loss\n",
    "        total_disc_loss = (loss_D_real + loss_D_fake) / 2\n",
    "\n",
    "        # backprop\n",
    "        total_disc_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        ################## Accumulate losses ###############\n",
    "\n",
    "        gen_loss += total_loss_generator.item()\n",
    "        disc_loss += total_disc_loss.item()\n",
    "\n",
    "        train_bar.set_postfix(\n",
    "            gen_loss=gen_loss / (batch_idx + 1), disc_loss=disc_loss / (batch_idx + 1)\n",
    "        )\n",
    "    \n",
    "    train_gen_loss.append(gen_loss / len(train_dataloader))\n",
    "    train_disc_loss.append(disc_loss / len(train_dataloader))\n",
    "\n",
    "    ############################ Testing ####################\n",
    "    gen_loss = 0\n",
    "    disc_loss = 0\n",
    "    valid_results = {'mse': 0, 'ssims': 0, 'psnr': 0, 'ssim': 0, 'batch_sizes': 0}\n",
    "\n",
    "    test_bar = tqdm(test_dataloader, desc=f\"Testing\")\n",
    "\n",
    "    for batch_idx, imgs in enumerate(test_bar):\n",
    "        generator.eval()\n",
    "        discriminator.eval()\n",
    "\n",
    "        # get the inputs\n",
    "        low_res_ipt = imgs[\"lr\"].to(cfg.device.device)\n",
    "        high_res_ipt = imgs[\"hr\"].to(cfg.device.device)\n",
    "\n",
    "        # get the batch size \n",
    "        batch_size = low_res_ipt.size(0)\n",
    "        valid_results['batch_sizes'] += batch_size\n",
    "\n",
    "\n",
    "        ############# Generator Eval ###############\n",
    "\n",
    "        generated_hr = generator(low_res_ipt)\n",
    "        disc_opt = discriminator(generated_hr)\n",
    "\n",
    "        # calculate the mse \n",
    "        batch_mse = ((generated_hr - high_res_ipt) ** 2).data.mean()\n",
    "        valid_results['mse'] += batch_mse * batch_size\n",
    "\n",
    "\n",
    "        batch_ssim = ssim(generated_hr, high_res_ipt).item()\n",
    "        valid_results['ssims'] += batch_ssim * batch_size\n",
    "        valid_results['psnr'] = 10 * math.log10((high_res_ipt.max()**2) / (valid_results['mse'] / valid_results['batch_sizes']))\n",
    "        valid_results['ssim'] = valid_results['ssims'] / valid_results['batch_sizes']\n",
    "\n",
    "\n",
    "\n",
    "        # Adverserial loss\n",
    "        loss_GAN = gan_loss(disc_opt, torch.ones_like(disc_opt))\n",
    "\n",
    "        # content loss\n",
    "        generated_features = feature_extractor(generated_hr)\n",
    "        real_feaures = feature_extractor(high_res_ipt)\n",
    "        loss_CONTENT = content_loss(generated_features, real_feaures)\n",
    "\n",
    "        # total loss\n",
    "        total_loss_generator = loss_CONTENT + 1e-3 * loss_GAN\n",
    "\n",
    "        #################### discriminator eval ######################\n",
    "\n",
    "        real_disc_opt = discriminator(high_res_ipt)\n",
    "        loss_D_real = gan_loss(real_disc_opt, torch.ones_like(real_disc_opt))\n",
    "\n",
    "        fake_disc_opt = discriminator(generated_hr.detach())\n",
    "        loss_D_fake = gan_loss(fake_disc_opt, torch.zeros_like(fake_disc_opt))\n",
    "\n",
    "        # total loss\n",
    "        total_disc_loss = (loss_D_real + loss_D_fake) / 2\n",
    "\n",
    "        ############### Accumulate losses ##########################\n",
    "        gen_loss += total_loss_generator.item()\n",
    "        disc_loss += total_disc_loss.item()\n",
    "\n",
    "        if epoch %  10 == 0:\n",
    "\n",
    "            if batch_idx % cfg.train.batch_size == 0:\n",
    "\n",
    "                imgs_lr = nn.functional.interpolate(low_res_ipt, scale_factor=4)\n",
    "                imgs_hr = make_grid(high_res_ipt, nrow=1, normalize=True)\n",
    "                gen_hr = make_grid(generated_hr, nrow=1, normalize=True)\n",
    "                imgs_lr = make_grid(imgs_lr, nrow=1, normalize=True)\n",
    "                img_grid = torch.cat((imgs_hr, imgs_lr, gen_hr), -1)\n",
    "                save_image(img_grid, f\"saved_models/srgan/images/{epoch}_{batch_idx}.png\", normalize=False)\n",
    "\n",
    "        test_bar.set_postfix(\n",
    "            gen_loss=gen_loss / (batch_idx + 1), disc_loss=disc_loss / (batch_idx + 1),\n",
    "            ssim = valid_results[\"ssim\"],\n",
    "            psnr = valid_results[\"psnr\"]\n",
    "        )\n",
    "    test_gen_loss.append(gen_loss / len(test_dataloader))\n",
    "    test_disc_loss.append(disc_loss / len(test_dataloader))\n",
    "    test_psnr.append(valid_results['psnr'])\n",
    "    test_ssim.append(valid_results['ssim'])\n",
    "\n",
    "\n",
    "    torch.save(generator.state_dict(), \"saved_models/srgan/generator.pth\")\n",
    "    torch.save(discriminator.state_dict(), \"saved_models/srgan/discriminator.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_term2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
