{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the SRGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# builtin \n",
    "import glob\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# all imports\n",
    "import torch \n",
    "import numpy as np \n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# our modules\n",
    "from src.config import cfg, root_path\n",
    "from src.utils import MeanSTDFinder\n",
    "from src.data_loaders import SuperResolutionDataLoader\n",
    "from src.models.srgan import Generator, Discriminator, VggFeatureExtractor\n",
    "\n",
    "\n",
    "# create path for models checkpoint\n",
    "Path(root_path).joinpath(\"saved_models/srgan\").mkdir(exist_ok=True, parents=True)\n",
    "Path(root_path).joinpath(\"saved_models/srgan/images\").mkdir(exist_ok=True, parents=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the images dataset path \n",
    "images_pth = cfg.dataset.images_dir\n",
    "\n",
    "train_paths, test_paths = train_test_split(\n",
    "    sorted(glob.glob(images_pth + \"/*.*\"))[:500],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# get the mean and std of the dataset \n",
    "# mean_std = MeanSTDFinder(images_dir=images_pth)()\n",
    "mean_std = {'mean': [0.2903465 , 0.31224626, 0.29810828],\n",
    " 'std': [0.1457739 , 0.13011318, 0.12317199]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataloaders\n",
    "train_dataloader = DataLoader(\n",
    "    SuperResolutionDataLoader(train_paths,**mean_std),\n",
    "    batch_size=cfg.train.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=cfg.train.n_cpu,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    SuperResolutionDataLoader(test_paths,**mean_std),\n",
    "    batch_size=int(cfg.train.batch_size * 0.75),\n",
    "    shuffle=True,\n",
    "    num_workers=cfg.train.n_cpu,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Define the Model Parameters ##########\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "feature_extractor = VggFeatureExtractor()\n",
    "feature_extractor.eval()\n",
    "\n",
    "gan_loss = torch.nn.BCEWithLogitsLoss()\n",
    "content_loss = torch.nn.L1Loss()\n",
    "\n",
    "\n",
    "# Transfer all to the device\n",
    "generator = generator.to(cfg.device.device)\n",
    "discriminator = discriminator.to(cfg.device.device)\n",
    "feature_extractor = feature_extractor.to(cfg.device.device)\n",
    "gan_loss = gan_loss.to(cfg.device.device)\n",
    "content_loss = content_loss.to(cfg.device.device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the optimizers for generator and discriminator\n",
    "\n",
    "optimizer_G = torch.optim.Adam(\n",
    "    generator.parameters(),\n",
    "    lr=cfg.train.learning_rate,\n",
    "    betas=(cfg.train.b1, cfg.train.b2),\n",
    ")\n",
    "optimizer_D = torch.optim.Adam(\n",
    "    discriminator.parameters(),\n",
    "    lr=cfg.train.learning_rate,\n",
    "    betas=(cfg.train.b1, cfg.train.b2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 100/100 [02:56<00:00,  1.76s/it, disc_loss=0.0616, gen_loss=1.24]\n",
      "Testing: 100%|██████████| 34/34 [00:15<00:00,  2.20it/s, disc_loss=2.14, gen_loss=1.25]\n",
      "Training:  36%|███▌      | 36/100 [01:06<01:57,  1.84s/it, disc_loss=0.000885, gen_loss=1.24]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 60\u001b[0m\n\u001b[1;32m     56\u001b[0m optimizer_D\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m################## Accumulate losses ###############\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m gen_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mtotal_loss_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m disc_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m total_disc_loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     63\u001b[0m train_bar\u001b[38;5;241m.\u001b[39mset_postfix(\n\u001b[1;32m     64\u001b[0m     gen_loss\u001b[38;5;241m=\u001b[39mgen_loss \u001b[38;5;241m/\u001b[39m (batch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), disc_loss\u001b[38;5;241m=\u001b[39mdisc_loss \u001b[38;5;241m/\u001b[39m (batch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     65\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train losses\n",
    "train_gen_loss, train_disc_loss, train_counter = [], [], []\n",
    "# test losses\n",
    "test_gen_loss, test_disc_loss = [], []\n",
    "\n",
    "\n",
    "for epoch in range(cfg.train.n_epochs):\n",
    "\n",
    "    ############################ Training ####################\n",
    "    gen_loss = 0\n",
    "    disc_loss = 0\n",
    "    train_bar = tqdm(train_dataloader, desc=f\"Training\")\n",
    "\n",
    "    for batch_idx, imgs in enumerate(train_bar):\n",
    "\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "\n",
    "        low_res_ipt = imgs[\"lr\"].to(cfg.device.device)\n",
    "        high_res_ipt = imgs[\"hr\"].to(cfg.device.device)\n",
    "        #################### Generator ######################\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "        generated_hr = generator(low_res_ipt)\n",
    "        disc_opt = discriminator(generated_hr)\n",
    "\n",
    "        # Adverserial loss\n",
    "        loss_GAN = gan_loss(disc_opt, torch.ones_like(disc_opt))\n",
    "\n",
    "        # content loss\n",
    "        generated_features = feature_extractor(generated_hr)\n",
    "        real_feaures = feature_extractor(high_res_ipt)\n",
    "        loss_CONTENT = content_loss(generated_features, real_feaures)\n",
    "\n",
    "        # total loss\n",
    "        total_loss_generator = loss_CONTENT + 1e-3 * loss_GAN\n",
    "\n",
    "        # backpropagate\n",
    "        total_loss_generator.backward()\n",
    "        optimizer_G.step()\n",
    "        #################### discriminator ######################\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        real_disc_opt = discriminator(high_res_ipt)\n",
    "        loss_D_real = gan_loss(real_disc_opt, torch.ones_like(real_disc_opt))\n",
    "\n",
    "        fake_disc_opt = discriminator(generated_hr.detach())\n",
    "        loss_D_fake = gan_loss(fake_disc_opt, torch.zeros_like(fake_disc_opt))\n",
    "\n",
    "        # total loss\n",
    "        total_disc_loss = (loss_D_real + loss_D_fake) / 2\n",
    "\n",
    "        # backprop\n",
    "        total_disc_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        ################## Accumulate losses ###############\n",
    "\n",
    "        gen_loss += total_loss_generator.item()\n",
    "        disc_loss += total_disc_loss.item()\n",
    "\n",
    "        train_bar.set_postfix(\n",
    "            gen_loss=gen_loss / (batch_idx + 1), disc_loss=disc_loss / (batch_idx + 1)\n",
    "        )\n",
    "    train_gen_loss.append(gen_loss / len(train_dataloader))\n",
    "    train_disc_loss.append(disc_loss / len(train_dataloader))\n",
    "\n",
    "    ############################ Testing ####################\n",
    "    gen_loss = 0\n",
    "    disc_loss = 0\n",
    "    test_bar = tqdm(test_dataloader, desc=f\"Testing\")\n",
    "\n",
    "    for batch_idx, imgs in enumerate(test_bar):\n",
    "        generator.eval()\n",
    "        discriminator.eval()\n",
    "\n",
    "        # get the inputs\n",
    "        low_res_ipt = imgs[\"lr\"].to(cfg.device.device)\n",
    "        high_res_ipt = imgs[\"hr\"].to(cfg.device.device)\n",
    "\n",
    "        ############# Generator Eval ###############\n",
    "\n",
    "        generated_hr = generator(low_res_ipt)\n",
    "        disc_opt = discriminator(generated_hr)\n",
    "\n",
    "        # Adverserial loss\n",
    "        loss_GAN = gan_loss(disc_opt, torch.ones_like(disc_opt))\n",
    "\n",
    "        # content loss\n",
    "        generated_features = feature_extractor(generated_hr)\n",
    "        real_feaures = feature_extractor(high_res_ipt)\n",
    "        loss_CONTENT = content_loss(generated_features, real_feaures)\n",
    "\n",
    "        # total loss\n",
    "        total_loss_generator = loss_CONTENT + 1e-3 * loss_GAN\n",
    "\n",
    "        #################### discriminator eval ######################\n",
    "\n",
    "        real_disc_opt = discriminator(high_res_ipt)\n",
    "        loss_D_real = gan_loss(real_disc_opt, torch.ones_like(real_disc_opt))\n",
    "\n",
    "        fake_disc_opt = discriminator(generated_hr.detach())\n",
    "        loss_D_fake = gan_loss(fake_disc_opt, torch.zeros_like(fake_disc_opt))\n",
    "\n",
    "        # total loss\n",
    "        total_disc_loss = (loss_D_real + loss_D_fake) / 2\n",
    "\n",
    "        ############### Accumulate losses ##########################\n",
    "        gen_loss += total_loss_generator.item()\n",
    "        disc_loss += total_disc_loss.item()\n",
    "\n",
    "        if random.uniform(0, 1) < 0.1:\n",
    "\n",
    "            imgs_lr = nn.functional.interpolate(low_res_ipt, scale_factor=4)\n",
    "            imgs_hr = make_grid(high_res_ipt, nrow=1, normalize=True)\n",
    "            gen_hr = make_grid(generated_hr, nrow=1, normalize=True)\n",
    "            imgs_lr = make_grid(imgs_lr, nrow=1, normalize=True)\n",
    "            img_grid = torch.cat((imgs_hr, imgs_lr, gen_hr), -1)\n",
    "            save_image(img_grid, f\"saved_models/srgan/images/{batch_idx}.png\", normalize=False)\n",
    "\n",
    "        test_bar.set_postfix(\n",
    "            gen_loss=gen_loss / (batch_idx + 1), disc_loss=disc_loss / (batch_idx + 1)\n",
    "        )\n",
    "    test_gen_loss.append(gen_loss / len(test_dataloader))\n",
    "    test_disc_loss.append(disc_loss / len(test_dataloader))\n",
    "\n",
    "    torch.save(generator.state_dict(), \"saved_models/srgan/generator.pth\")\n",
    "    torch.save(discriminator.state_dict(), \"saved_models/srgan/discriminator.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_term2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
